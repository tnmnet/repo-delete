# Verify configuration parameters specific for AWS as a sanity check and fail-fast.
# Parameters:
#   1) path of the configuration file
#   2) path of the secrets configuration file
#   3) workspace label
# Return:
#   sets global constants
verify_configuration ()
{
  # Check presence of CLI
  verify_tool "aws"

  verify_conf_value $CONFIG_FILE "workspaces.(name==$WORKSPACE).kubernetes.version"
  K8S_VERSION=$verify_conf_value_retval

  verify_conf_value $CONFIG_FILE "workspaces.(name==$WORKSPACE).kubernetes.kops-state-store-prefix"
  KOPS_STATE_BUCKET_NAME_PREFIX=$verify_conf_value_retval
  KOPS_STATE_BUCKET_NAME="$KOPS_STATE_BUCKET_NAME_PREFIX-$WORKSPACE"
  export KOPS_STATE_STORE="s3://$KOPS_STATE_BUCKET_NAME"

  verify_conf_value $CONFIG_FILE "workspaces.(name==$WORKSPACE).kubernetes.ingress-dns-zone"
  DNS_ZONE=$verify_conf_value_retval
  export KOPS_CLUSTER_NAME="$WORKSPACE.$DNS_ZONE"

  CLUSTER_CONFIG="$DIR_LOCAL_CACHE_WKS/cluster-config.$KOPS_CLUSTER_NAME.yaml"
  KEY_PAIR_NAME="key-pair.$KOPS_CLUSTER_NAME"

  return 0
}


# Executes the script specific for AWS to create the Terraform backend storage.
# Make sure variables are set accordingly.
build_kubernetes ()
{
  # Create the bucket
  echo "Creating 'kops' state storage..."
  aws s3api create-bucket \
      --bucket $KOPS_STATE_BUCKET_NAME \
      --region $AWS_DEFAULT_REGION \
      --create-bucket-configuration LocationConstraint=$AWS_DEFAULT_REGION

  aws s3api put-bucket-versioning \
          --bucket $KOPS_STATE_BUCKET_NAME \
          --versioning-configuration Status=Enabled
  echo "Creating 'kops' state storage...done."

  echo "Generating key pair..."
  ssh-keygen -b 2048 -t rsa -f $DIR_LOCAL_CACHE_WKS/$KEY_PAIR_NAME.secrets -q -N ""
  echo "Generating key pair...done."

  echo "Creating cluster '$KOPS_CLUSTER_NAME'..."

  # First create cluster manifest for visibility and later use/modifications
  # Availability zone 'b' below is chosen randomly
  echo "Creating cluster manifest..."
  kops create cluster \
      --kubernetes-version "$K8S_VERSION" \
      --zones ${AWS_DEFAULT_REGION}b \
      --node-count 2 \
      --ssh-public-key "$DIR_LOCAL_CACHE_WKS/$KEY_PAIR_NAME.secrets.pub" \
      --dry-run \
      --output yaml > $CLUSTER_CONFIG 

  # Now creating cluster configuration...
  kops create -f $CLUSTER_CONFIG
  # Allow time to catch up
  sleep 3
  echo "Creating cluster manifest...done."

  # ... and finally updating the cluster cloud resources (i.e. creating in this case)
  echo "Creating cloud resources..."
  kops create secret sshpublickey admin -i $DIR_LOCAL_CACHE_WKS/$KEY_PAIR_NAME.secrets.pub
  KUBECONFIG=$DIR_LOCAL_CACHE_WKS/kube-config kops update cluster --yes
  # Note: for updating one also needs to roll out the changes to match configuration:
  #kops rolling-update cluster --yes
  
  # Allow time to catch up
  sleep 3
  echo "Creating cloud resources...done."

  # Wait for kube-config to be ready before proceeding
  echo "Waiting for 'kube-config' to be available..."
  declare -i counter=0
  while [ ! -f $DIR_LOCAL_CACHE_WKS/kube-config ]
  do
    counter=$((counter+1))
    echo "Still waiting... ($counter)"
    sleep 10
  done
  echo "Waiting for 'kube-config' to be available...done."

  # Wait for the cluster to be fully initialized
  declare -i wait=12          # in minutes
  declare -i sleeptime=10     # in seconds
  echo "Waiting for the Kubernetes cluster to be available (about $wait minutes)..."
  declare -i max_counter=$(($wait * 60 / $sleeptime))
  declare -i counter=0
  while [[ $counter -lt $max_counter ]]; do
    counter=$((counter+1))
    echo "Still waiting... ($counter/$max_counter)"
    sleep $sleeptime
  done
  echo "Waiting for the Kubernetes cluster to be available...done."

  echo $'\e[1;34m'SSH key pair for cluster access in: $DIR_LOCAL_CACHE_WKS$'\e[0m'

  return 0
}

# Destroys the backed in AWS
destroy_kubernetes ()
{
  kops delete cluster --yes
  rm $CLUSTER_CONFIG

  echo "Removing kops state store 1/3..."
  # Switch off versioning
  aws s3api put-bucket-versioning \
          --bucket "$KOPS_STATE_BUCKET_NAME" \
          --versioning-configuration Status=Suspended

  # Delete all Versions
  BUCKET_ITEMS=$(aws s3api list-object-versions \
        --bucket "$KOPS_STATE_BUCKET_NAME" \
        --no-paginate \
        --output=json \
        --query='{Versions: Versions[].{Key:Key,VersionId:VersionId}}' \
        )
  # Replace 'Versions' with 'Objects' for subsequent operations
  BUCKET_ITEM_PARAM=$(echo $BUCKET_ITEMS | sed 's/Versions/Objects/g')
  # Silence the object deletion unless there is an error
  BUCKET_ITEM_PARAM=$(echo $BUCKET_ITEM_PARAM | sed 's/]/],\"Quiet\":true/')
  # Finally, delete
  aws s3api delete-objects \
      --bucket "$KOPS_STATE_BUCKET_NAME" \
      --delete "$BUCKET_ITEM_PARAM"
  echo "Removing kops state store 1/3...done."

  echo "Removing kops state store 2/3..."
  # Delete all DeleteMarkers
  BUCKET_ITEMS=$(aws s3api list-object-versions \
        --bucket "$KOPS_STATE_BUCKET_NAME" \
        --no-paginate \
        --output=json \
        --query='{DeleteMarkers: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' \
        )
  # Replace 'DeleteMarkers' with 'Objects' for subsequent operations
  BUCKET_ITEM_PARAM=$(echo $BUCKET_ITEMS | sed 's/DeleteMarkers/Objects/g')
  # Silence the object deletion unless there is an error
  BUCKET_ITEM_PARAM=$(echo $BUCKET_ITEM_PARAM | sed 's/]/],\"Quiet\":true/')
  # ... and delete
  aws s3api delete-objects \
      --bucket "$KOPS_STATE_BUCKET_NAME" \
      --delete "$BUCKET_ITEM_PARAM"
  echo "Removing kops state store 2/3...done."

  # And, finally, take out the bucket
  echo "Removing kops state store 3/3..."
  aws s3api delete-bucket --bucket "$KOPS_STATE_BUCKET_NAME"
  echo "Removing kops state store 3/3...done."

  # Remove SSH key pair
  rm -rf $DIR_LOCAL_CACHE_WKS/$KEY_PAIR_NAME.secrets*

  return 0
}
